{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c20dee3",
   "metadata": {},
   "source": [
    "# Atividade 03\n",
    "Com base no código de Image Retrieval apresentado durante a aula, modifique o código para que a imagem seja representada com os keypoints definidos em grid ao invés de usar detector de keypoints.\n",
    "\n",
    "1. Use a função da última atividade prática para definir kepoints em grid e keypoints de forma aleatória.\n",
    "2. Execute o código e calcule a acurácia média.\n",
    "3. Compare a acurácia média com a acurácia média do código apresentado durante a aula.\n",
    "4. Escreva uma pequena discussão (máximo 5 linhas) explicado o resultado encontrado.\n",
    "5. Use a partição de validação para encontrar o melhor tamanho do grid através do teste do cotovelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e75ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/luisa/Documents/faculdade/optativas/inf 492/inf492/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in /home/luisa/Documents/faculdade/optativas/inf 492/inf492/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/luisa/Documents/faculdade/optativas/inf 492/inf492/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/luisa/Documents/faculdade/optativas/inf 492/inf492/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/luisa/Documents/faculdade/optativas/inf 492/inf492/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/luisa/Documents/faculdade/optativas/inf 492/inf492/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn & pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95829699",
   "metadata": {},
   "source": [
    "# Função para definir keypoints aleatoriamente e em grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c9e2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "def random_keypoints(image, number_keypoints):\n",
    "    keypoints =[]\n",
    "    height, width = image.shape\n",
    "    for i in range(number_keypoints):\n",
    "        keypoint = cv.KeyPoint()\n",
    "        h = random.randint(0, height - 1)\n",
    "        w = random.randint(0, width - 1)\n",
    "        keypoint.pt = (h,w)\n",
    "        keypoint.size = 40\n",
    "        keypoints.append(keypoint)\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def grid_keypoints(image, grid_size):\n",
    "    keypoints = []\n",
    "    height, width = image.shape\n",
    "    keypoints = []\n",
    "    for i in range(width): \n",
    "        if(i%grid_size == 0):\n",
    "            for j in range(height): #(0,0), (0,grid_size), ... , (grid_size,0), (grid_size,grid_size), ...\n",
    "                if(j % grid_size == 0):\n",
    "                    keypoint = cv.KeyPoint()\n",
    "                    keypoint.pt = (i, j)\n",
    "                    keypoint.size = grid_size/2\n",
    "                    keypoints.append(keypoint)\n",
    "    return keypoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ef392",
   "metadata": {},
   "source": [
    "# Código usado em aula prática com modificações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebb6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2214f",
   "metadata": {},
   "source": [
    "# Image Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04288e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_keypoints( image , kps ) :\n",
    "    cv2.drawKeypoints( image, kps, image, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS )\n",
    "\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.imshow(image, aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.title('Keypoints and descriptors.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433bac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_images ( dataset_path, indices , id_test , ids , labels ) :\n",
    "    \n",
    "    label = (ids[id_test] - 1) // 80\n",
    "    name = dataset_path + '/jpg/' + str(label) + '/image_' + str(ids[id_test]).zfill(4) + '.jpg'\n",
    "    \n",
    "    image = cv2.imread( name )\n",
    "    image = cv2.cvtColor( image , cv2.COLOR_BGR2RGB )\n",
    "    \n",
    "    top = 0\n",
    "    show_image_label(top, image, labels[id_test], ids[id_test] )\n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    for i in indices[0] :\n",
    "        label_i = labels[i]\n",
    "        name = dataset_path + '/jpg/' + str(label_i) + '/image_' + str(ids[i]).zfill(4) + '.jpg'\n",
    "\n",
    "        image = cv2.imread( name )\n",
    "        image = cv2.cvtColor( image , cv2.COLOR_BGR2RGB )\n",
    "\n",
    "        show_image_label(top, image, label_i, ids[i] )   \n",
    "        top = top + 1\n",
    "        \n",
    "    \n",
    "def show_image_label ( top, image, label , image_id ) :\n",
    "    \n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(image, aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'{top} - Image id {image_id} with label {label}.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4101d4",
   "metadata": {},
   "source": [
    "# Generate descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa466267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_describe_keypoints ( image, algorithm_descriptor='orb', algorithm_detector='orb', grid_size=15) :\n",
    "    \n",
    "    image_gray = cv2.cvtColor( image , cv2.COLOR_BGR2GRAY )\n",
    "        \n",
    "    if algorithm_descriptor == 'sift' :\n",
    "        keypoint = sift = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    elif algorithm_descriptor == 'orb' :\n",
    "        keypoint = cv2.ORB_create()\n",
    "        \n",
    "    #adding random and grid algorithms\n",
    "    if algorithm_detector == 'sift' or algorithm_detector == 'orb':\n",
    "        kps = keypoint.detect( image_gray, None ) \n",
    "    elif algorithm_detector == 'random':\n",
    "        kps = random_keypoints(image_gray, 300)\n",
    "    elif algorithm_detector == 'grid':\n",
    "        kps = grid_keypoints(image_gray, grid_size)\n",
    "   \n",
    "    else :\n",
    "        print('Error: algorithm not defined')\n",
    "        return None   \n",
    "\n",
    "    # Describing Keypoints\n",
    "    kps, descs = keypoint.compute( image_gray, kps )\n",
    "    \n",
    "    return kps, descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad07433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bovw_descriptors (image, dictionary, algorithm_descriptor='orb', algorithm_detector='orb', grid_size=15) :\n",
    "    \n",
    "    descs = detect_and_describe_keypoints( image, algorithm_descriptor, algorithm_detector, grid_size)[1]\n",
    "\n",
    "    predicted = dictionary.predict(np.array(descs, dtype=np.double))\n",
    "    \n",
    "    desc_bovw = np.histogram(predicted, bins=range(0, dictionary.n_clusters+1))[0]\n",
    "    \n",
    "    return desc_bovw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94f5ce",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8848b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "def create_dictionary_kmeans ( vocabulary , num_cluster ) :\n",
    "  \n",
    "    print( ' -> [I] Dictionary Info:\\n', \n",
    "        '\\nTrain len: ', len(vocabulary),\n",
    "        '\\nDimension: ', len(vocabulary[0]),\n",
    "        '\\nClusters: ', num_cluster \n",
    "        )\n",
    "\n",
    "#     dictionary = KMeans( n_clusters=num_cluster )\n",
    "    dictionary = MiniBatchKMeans( n_clusters=num_cluster, batch_size=1000)\n",
    "\n",
    "    print ( 'Learning dictionary by Kmeans...')\n",
    "    dictionary = dictionary.fit( vocabulary )\n",
    "    print ( 'Done.')\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af33191",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda13d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import tqdm\n",
    "\n",
    "def create_vocabulary ( dataset_path , algorithm_descriptor='orb', algorithm_detector='orb', grid_size=15,\n",
    "                       show_image=False ,  debug=False ) :\n",
    "\n",
    "    mat = scipy.io.loadmat( dataset_path+'/datasplits.mat' )\n",
    "\n",
    "    ids = mat['trn1'][0] #  'val1' or 'tst1' \n",
    "    \n",
    "    if algorithm_descriptor == 'orb' :\n",
    "        train_descs = np.ndarray( shape=(0,32) , dtype=float )\n",
    "    elif algorithm_descriptor == 'sift': \n",
    "        train_descs = np.ndarray( shape=(0,128) , dtype=float )\n",
    "    else :\n",
    "        print('Error:Algorithm not defined.')\n",
    "        return None\n",
    "    cont = 0\n",
    "\n",
    "    for id in tqdm.tqdm(ids, desc='Processing train set') :\n",
    "\n",
    "        label = (id - 1) // 80\n",
    "        name = dataset_path + '/jpg/' + str(label) + '/image_' + str(id).zfill(4) + '.jpg'\n",
    "\n",
    "        image = cv2.imread( name )\n",
    "        \n",
    "        if image is None:\n",
    "            print(f'Reading image Error. Path: {name}')\n",
    "            return None\n",
    "\n",
    "        kps, descs = detect_and_describe_keypoints ( image, algorithm_descriptor, algorithm_detector, grid_size )\n",
    "         \n",
    "        train_descs = np.concatenate((train_descs, descs), axis=0)\n",
    "        \n",
    "        if show_image :\n",
    "            show_image_and_keypoints(image, kps)\n",
    "\n",
    "        if debug :\n",
    "            print( name )\n",
    "            print( 'Number of keypoints: ', len(kps) )\n",
    "            print( 'Number of images: ', len(ids) )\n",
    "            print( 'Descriptor size: ', len(descs[0]) )\n",
    "            print( type(descs[0]) )\n",
    "      \n",
    "    print( ' -> [I] Image Loader Info:\\n',       \n",
    "      '\\nTrain len: ', len(train_descs),\n",
    "      '\\nNumber of images: ', len(ids),\n",
    "      '\\nDescriptor size: ', len(descs[0])      \n",
    "      )\n",
    "    \n",
    "    return train_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff15234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_dataset( dataset_path, dictionary , algorithm_descriptor='orb', algorithm_detector='orb', ids='tst1'\n",
    "                      , grid_size=15 ) :\n",
    "\n",
    "\n",
    "    mat = scipy.io.loadmat( dataset_path+'/datasplits.mat' )\n",
    "\n",
    "    ids = mat[ids][0] #  'trn1' or 'val1' \n",
    "    \n",
    "    space = []\n",
    "    labels = []\n",
    "    \n",
    "    for id in tqdm.tqdm(ids, desc='Processing train set') :\n",
    "\n",
    "        label = (id - 1) // 80\n",
    "        name = dataset_path + '/jpg/' + str(label) + '/image_' + str(id).zfill(4) + '.jpg'\n",
    "\n",
    "        image = cv2.imread( name )\n",
    "\n",
    "        desc_bovw = create_bovw_descriptors(image, dictionary, algorithm_descriptor, algorithm_detector, grid_size)\n",
    "\n",
    "        space.append(desc_bovw)\n",
    "        labels.append(label)\n",
    "        \n",
    "    print( ' -> [I] Space Describing Info:\\n', \n",
    "        '\\nNumber of images: ', len(space), \n",
    "        '\\nNumber of labels: ', len(labels),\n",
    "        '\\nDimension: ', len(space[0])\n",
    "        )\n",
    "\n",
    "    return space , labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b8b6e",
   "metadata": {},
   "source": [
    "# Experimental evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42375af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def run_test ( space , labels , dictionary , dataset_path,algorithm_descriptor='orb', algorithm_detector='orb',ids='tst1',\n",
    "              grid_size=15, top=10 ) :\n",
    "    knn = NearestNeighbors(n_neighbors=top+1).fit(space)\n",
    "    \n",
    "    mat = scipy.io.loadmat( dataset_path+'/datasplits.mat' )\n",
    "\n",
    "    ids = mat[ids][0] #  'trn1' or 'val1'\n",
    "    \n",
    "    accuracy_t = 0\n",
    "    \n",
    "    for id_test in tqdm.tqdm(ids, desc='running the test phase') :\n",
    "        \n",
    "        label = (id_test - 1) // 80\n",
    "        name = dataset_path + '/jpg/' + str(label) + '/image_' + str(id_test).zfill(4) + '.jpg'\n",
    "\n",
    "        image = cv2.imread( name )\n",
    "        \n",
    "        desc_bovw = create_bovw_descriptors(image, dictionary, algorithm_descriptor, algorithm_detector, grid_size)\n",
    "\n",
    "        indices = knn.kneighbors(desc_bovw.reshape(1, -1))[1]\n",
    "\n",
    "        labels_top = [ labels[i] for i in indices[0] ]\n",
    "\n",
    "        accuracy = sum( np.equal(labels_top, label) )\n",
    "        accuracy =( (accuracy-1)/(top) ) * 100 \n",
    "        accuracy_t = accuracy_t + accuracy\n",
    "        \n",
    "    print(f'Average accuracy in the test set: {accuracy_t/len(ids):5.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ac2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_single_image ( space , labels , dictionary , dataset_path, algorithm_descriptor='orb', algorithm_detector='orb',\n",
    "                           grid_size = 15 , top=10 ) :\n",
    "    knn = NearestNeighbors(n_neighbors=top+1).fit(space)\n",
    "    \n",
    "    mat = scipy.io.loadmat( dataset_path+'/datasplits.mat' )\n",
    "\n",
    "    ids = mat['tst1'][0] #  'trn1' or 'val1'\n",
    "    \n",
    "    id_test = random.randrange( len(ids) )\n",
    "        \n",
    "    label = (ids[id_test] - 1) // 80\n",
    "    name = dataset_path + '/jpg/' + str(label) + '/image_' + str(ids[id_test]).zfill(4) + '.jpg'\n",
    "    \n",
    "    image = cv2.imread( name )\n",
    "\n",
    "    desc_bovw = create_bovw_descriptors(image, dictionary, algorithm_descriptor, algorithm_detector, grid_size)\n",
    "    \n",
    "    distances, indices = knn.kneighbors(desc_bovw.reshape(1, -1))\n",
    "    \n",
    "    show_top_images(dataset_path, indices, id_test, ids, labels)\n",
    "    \n",
    "    labels_top = [ labels[i] for i in indices[0] ]\n",
    "    \n",
    "    accuracy = sum( np.equal( label , labels_top ) )\n",
    "    accuracy =( (accuracy-1)/(top) ) * 100 \n",
    "    \n",
    "    print(f'Accuracy for image id {ids[id_test]}: {accuracy:5.2f}%')\n",
    "    \n",
    "    print(name)    \n",
    "    print(f'Image: {ids[id_test]} with label {labels[id_test]}')    \n",
    "    print(f'Closest image: {ids[indices[0][0]]} with distance {distances[0][0]} and label {labels[indices[0][0]]}')\n",
    "    print('Distances: ',distances)\n",
    "    print('Indices: ',indices)\n",
    "    print('Labels: ',labels_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939449e",
   "metadata": {},
   "source": [
    "# Execution ORB detector and SIFT detector(during class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c2a0c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set:   0%|                             | 0/680 [00:00<?, ?it/s][ WARN:0@17.671] global shadow_sift.hpp:13 SIFT_create DEPRECATED: cv.xfeatures2d.SIFT_create() is deprecated due SIFT tranfer to the main repository. https://github.com/opencv/opencv/issues/16736\n",
      "Processing train set: 100%|███████████████████| 680/680 [02:02<00:00,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Image Loader Info:\n",
      " \n",
      "Train len:  1233814 \n",
      "Number of images:  680 \n",
      "Descriptor size:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'flowers_classes'\n",
    "algorithm_descriptor = 'sift'\n",
    "algorithm_detector = 'sift'\n",
    "vocabulary = create_vocabulary( dataset_path, algorithm_descriptor, algorithm_detector ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c11472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Dictionary Info:\n",
      " \n",
      "Train len:  1233814 \n",
      "Dimension:  128 \n",
      "Clusters:  100\n",
      "Learning dictionary by Kmeans...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|███████████████████| 340/340 [00:29<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Space Describing Info:\n",
      " \n",
      "Number of images:  340 \n",
      "Number of labels:  340 \n",
      "Dimension:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running the test phase: 100%|█████████████████| 340/340 [00:48<00:00,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy in the test set: 21.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "dictionary = create_dictionary_kmeans( vocabulary , num_clusters )\n",
    "space, labels = represent_dataset ( dataset_path , dictionary, algorithm_descriptor, algorithm_detector,ids='tst1' )\n",
    "run_test ( space , labels , dictionary , dataset_path, algorithm_descriptor, algorithm_detector, ids='tst1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14496b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|███████████████████| 680/680 [00:10<00:00, 64.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Image Loader Info:\n",
      " \n",
      "Train len:  333473 \n",
      "Number of images:  680 \n",
      "Descriptor size:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'flowers_classes'\n",
    "algorithm_descriptor = 'orb'\n",
    "algorithm_detector = 'orb'\n",
    "vocabulary = create_vocabulary( dataset_path, algorithm_descriptor, algorithm_detector ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d490e708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Dictionary Info:\n",
      " \n",
      "Train len:  333473 \n",
      "Dimension:  32 \n",
      "Clusters:  100\n",
      "Learning dictionary by Kmeans...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|██████████████████| 340/340 [00:02<00:00, 115.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Space Describing Info:\n",
      " \n",
      "Number of images:  340 \n",
      "Number of labels:  340 \n",
      "Dimension:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running the test phase: 100%|█████████████████| 340/340 [00:13<00:00, 25.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy in the test set: 16.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "dictionary = create_dictionary_kmeans( vocabulary , num_clusters )\n",
    "space, labels = represent_dataset ( dataset_path , dictionary, algorithm_descriptor, algorithm_detector,ids='tst1' )\n",
    "run_test ( space , labels , dictionary , dataset_path, algorithm_descriptor, algorithm_detector, ids='tst1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a1a23",
   "metadata": {},
   "source": [
    "# Execution with random detector\n",
    "Selected 300 random keypoints for each image, like in short activity 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c777dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|██████████████████| 680/680 [00:03<00:00, 193.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Image Loader Info:\n",
      " \n",
      "Train len:  132043 \n",
      "Number of images:  680 \n",
      "Descriptor size:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'flowers_classes'\n",
    "algorithm_descriptor = 'orb'\n",
    "algorithm_detector = 'random'\n",
    "vocabulary = create_vocabulary( dataset_path, algorithm_descriptor, algorithm_detector ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20e05dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Dictionary Info:\n",
      " \n",
      "Train len:  132043 \n",
      "Dimension:  32 \n",
      "Clusters:  100\n",
      "Learning dictionary by Kmeans...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:01<00:00, 254.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Space Describing Info:\n",
      " \n",
      "Number of images:  340 \n",
      "Number of labels:  340 \n",
      "Dimension:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running the test phase: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:03<00:00, 93.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy in the test set:  3.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "dictionary = create_dictionary_kmeans( vocabulary , num_clusters )\n",
    "space, labels = represent_dataset ( dataset_path , dictionary, algorithm_descriptor, algorithm_detector,ids='tst1' )\n",
    "run_test ( space , labels , dictionary , dataset_path, algorithm_descriptor, algorithm_detector, ids='tst1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473705a",
   "metadata": {},
   "source": [
    "#### Using RANDOM method to detect keypoints and ORB to describe them, with a number of cluster of 100, using a random algorithm for detect keypoints, we got an average accuracy of 3.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4de5dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 680/680 [00:52<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Image Loader Info:\n",
      " \n",
      "Train len:  204000 \n",
      "Number of images:  680 \n",
      "Descriptor size:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'flowers_classes'\n",
    "algorithm_descriptor = 'sift'\n",
    "algorithm_detector = 'random'\n",
    "vocabulary = create_vocabulary( dataset_path, algorithm_descriptor, algorithm_detector ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b77ae091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Dictionary Info:\n",
      " \n",
      "Train len:  204000 \n",
      "Dimension:  128 \n",
      "Clusters:  100\n",
      "Learning dictionary by Kmeans...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:21<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Space Describing Info:\n",
      " \n",
      "Number of images:  340 \n",
      "Number of labels:  340 \n",
      "Dimension:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running the test phase: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:31<00:00, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy in the test set: 16.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "dictionary = create_dictionary_kmeans( vocabulary , num_clusters )\n",
    "space, labels = represent_dataset ( dataset_path , dictionary, algorithm_descriptor, algorithm_detector,ids='tst1' )\n",
    "run_test ( space , labels , dictionary , dataset_path, algorithm_descriptor, algorithm_detector, ids='tst1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a4aac",
   "metadata": {},
   "source": [
    "#### Using RANDOM method to detect keypoints and SIFT to describe them, with a number of cluster of 100, using a random algorithm for detect keypoints, we got an average accuracy of 16.71%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40641984",
   "metadata": {},
   "source": [
    "# Execution with grid detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d53e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'flowers_classes'\n",
    "algorithm_descriptor = 'sift'\n",
    "algorithm_detector = 'grid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2a516a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 680/680 [00:20<00:00, 32.66it/s]\n",
      "/home/luisa/Documents/faculdade/optativas/inf 492/inf492/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Image Loader Info:\n",
      " \n",
      "Train len:  68940 \n",
      "Number of images:  680 \n",
      "Descriptor size:  128\n",
      " -> [I] Dictionary Info:\n",
      " \n",
      "Train len:  68940 \n",
      "Dimension:  128 \n",
      "Clusters:  100\n",
      "Learning dictionary by Kmeans...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:09<00:00, 35.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Space Describing Info:\n",
      " \n",
      "Number of images:  340 \n",
      "Number of labels:  340 \n",
      "Dimension:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running the test phase: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:24<00:00, 14.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy in the test set: 17.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "grid_size = 45\n",
    "vocabulary = create_vocabulary( dataset_path, algorithm_descriptor, algorithm_detector, grid_size ) \n",
    "\n",
    "dictionary = create_dictionary_kmeans( vocabulary , num_clusters )\n",
    "space, labels = represent_dataset ( dataset_path , dictionary, algorithm_descriptor, algorithm_detector,\n",
    "                                   ids='val1', grid_size=grid_size)\n",
    "run_test ( space , labels , dictionary , dataset_path, algorithm_descriptor, algorithm_detector,\n",
    "          ids='val1', grid_size=grid_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596d80a",
   "metadata": {},
   "source": [
    "## Use the validation partition to find the best grid size through the elbow test\n",
    "As the grid size increases, the number of detectable keypoints decreases. To perform the elbow test, I experimented with seven different grid sizes.\n",
    "| Grid Size | Accuracy |\n",
    "| -------- | -------- |\n",
    "| 150x150 | 15.02%|\n",
    "| 100x100 | 16.21%|\n",
    "| 90x90 | 16.53%|\n",
    "| 60x60 |17.03%|\n",
    "| 45x45 | 18.21%|\n",
    "| 30x30 | 16.56%|\n",
    "| 15x15 | 16.06% |\n",
    "\n",
    "Based on the results, it is evident that the highest accuracy is achieved with a grid size of 45x45. And, as we move beyond this size, the accuracy starts to decline. Therefore, we can infer that 45x45 is the optimal grid size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d068118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|███████████████████| 680/680 [00:20<00:00, 32.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Image Loader Info:\n",
      " \n",
      "Train len:  119412 \n",
      "Number of images:  680 \n",
      "Descriptor size:  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'flowers_classes'\n",
    "algorithm_descriptor = 'sift'\n",
    "algorithm_detector = 'grid'\n",
    "vocabulary = create_vocabulary( dataset_path, algorithm_descriptor, algorithm_detector, grid_size=45 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "061b361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Dictionary Info:\n",
      " \n",
      "Train len:  119412 \n",
      "Dimension:  128 \n",
      "Clusters:  100\n",
      "Learning dictionary by Kmeans...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train set: 100%|███████████████████| 340/340 [00:08<00:00, 40.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> [I] Space Describing Info:\n",
      " \n",
      "Number of images:  340 \n",
      "Number of labels:  340 \n",
      "Dimension:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running the test phase: 100%|█████████████████| 340/340 [00:23<00:00, 14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy in the test set: 16.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "dictionary = create_dictionary_kmeans( vocabulary , num_clusters )\n",
    "space, labels = represent_dataset ( dataset_path , dictionary, algorithm_descriptor, algorithm_detector,ids='tst1', grid_size=45)\n",
    "run_test ( space , labels , dictionary , dataset_path, algorithm_descriptor, algorithm_detector, ids='tst1', grid_size=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd3f4b8",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "The accuracy achieved through the SIFT and ORB detectors used in class is significantly higher than that achieved through the random and grid detector methods. This can be attributed to the fact that the latter methods often fail to identify a unique, informative, and unambiguous part of the image as a keypoint. Moreover, the SIFT descriptor consistently outperforms the ORB descriptor with all keypoint detection methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
